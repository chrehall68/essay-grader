{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Essay Grader"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The goal of this is to have the AI output whether the essay is good or bad on a scale from 1 to 10."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "import tensorflow as tf\n",
            "import keras\n",
            "import keras.layers as layers\n",
            "import numpy as np\n",
            "import os\n",
            "import pickle"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Load Glove"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "PATH_TO_GLOVE_FILE = \"./glove.6B.100d.txt\"\n",
            "GLOVE_OUTPUT_DIM = 100"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "loading glove from scratch\n",
                  "Found 400000 word vectors.\n"
               ]
            }
         ],
         "source": [
            "embeddings = {}\n",
            "\n",
            "if not os.path.exists(\"./processed_glove.b\"):\n",
            "    print(\"loading glove from scratch\")\n",
            "    with open(PATH_TO_GLOVE_FILE) as f:\n",
            "        for line in f:\n",
            "            word, coefs = line.split(maxsplit=1)\n",
            "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
            "            embeddings[word] = coefs\n",
            "\n",
            "    print(\"Found %s word vectors.\" % len(embeddings))\n",
            "\n",
            "    with open(\"./processed_glove.b\", \"wb\") as file:\n",
            "        pickle.dump(embeddings, file)\n",
            "        file.close()\n",
            "else:\n",
            "    print(\"loading preprocessed glove\")\n",
            "    with open(\"./processed_glove.b\", \"rb\") as file:\n",
            "        embeddings = pickle.load(file)\n",
            "        file.close()\n",
            "    \n",
            "    print(\"Found %s word vectors.\" % len(embeddings))\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Get the Text Vectorizer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "2022-10-12 21:31:45.627088: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
                  "2022-10-12 21:31:45.627388: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-17QUFAA): /proc/driver/nvidia/version does not exist\n",
                  "2022-10-12 21:31:45.636577: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
                  "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
               ]
            }
         ],
         "source": [
            "vectorizer = layers.TextVectorization(len(embeddings))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "# quickly learn the words (should take about 40 seconds)\n",
            "vectorizer_batch_size = 10\n",
            "quick_dataset = tf.data.Dataset.from_tensor_slices(np.array(list(embeddings.keys()))).batch(vectorizer_batch_size)\n",
            "vectorizer.adapt(quick_dataset, steps=len(embeddings)/vectorizer_batch_size)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "<tf.Tensor: shape=(1, 7), dtype=int64, numpy=array([[  8103,  91227,   2926, 346554,   2926,  29418, 294322]])>"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# try it out\n",
            "vectorizer([\"I saw it, and it was cool.\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "# make it non-trainable\n",
            "vectorizer.trainable = False\n",
            "\n",
            "# get vocabulary\n",
            "voc = vectorizer.get_vocabulary()\n",
            "word_index = dict(zip(voc, range(len(voc))))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Make Embedding Layer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Converted 336326 words (38711 misses)\n"
               ]
            }
         ],
         "source": [
            "num_tokens = len(voc) + 2\n",
            "hits = 0\n",
            "misses = 0\n",
            "\n",
            "# Prepare embedding matrix\n",
            "embedding_matrix = np.zeros((num_tokens, GLOVE_OUTPUT_DIM))\n",
            "for word, i in word_index.items():\n",
            "    embedding_vector = embeddings.get(word)\n",
            "    if embedding_vector is not None:\n",
            "        # Words not found in embedding index will be all-zeros.\n",
            "        # This includes the representation for \"padding\" and \"OOV\"\n",
            "        embedding_matrix[i] = embedding_vector\n",
            "        hits += 1\n",
            "    else:\n",
            "        misses += 1\n",
            "print(\"Converted %d words (%d misses)\" % (hits, misses))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [],
         "source": [
            "glove = keras.layers.Embedding(\n",
            "    num_tokens,\n",
            "    GLOVE_OUTPUT_DIM,\n",
            "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
            "    trainable=False,\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Compare Glove Embeddings Layer and Raw Glove"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "<tf.Tensor: shape=(1, 1, 100), dtype=float32, numpy=\n",
                     "array([[[-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,\n",
                     "          0.043953, -0.39141 ,  0.3344  , -0.57545 ,  0.087459,\n",
                     "          0.28787 , -0.06731 ,  0.30906 , -0.26384 , -0.13231 ,\n",
                     "         -0.20757 ,  0.33395 , -0.33848 , -0.31743 , -0.48336 ,\n",
                     "          0.1464  , -0.37304 ,  0.34577 ,  0.052041,  0.44946 ,\n",
                     "         -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
                     "         -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,\n",
                     "          0.086173,  0.20397 ,  0.52624 ,  0.17164 , -0.082378,\n",
                     "         -0.71787 , -0.41531 ,  0.20335 , -0.12763 ,  0.41367 ,\n",
                     "          0.55187 ,  0.57908 , -0.33477 , -0.36559 , -0.54857 ,\n",
                     "         -0.062892,  0.26584 ,  0.30205 ,  0.99775 , -0.80481 ,\n",
                     "         -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
                     "         -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  ,\n",
                     "         -0.19358 , -0.074575,  0.23353 , -0.052062, -0.22044 ,\n",
                     "          0.057162, -0.15806 , -0.30798 , -0.41625 ,  0.37972 ,\n",
                     "          0.15006 , -0.53212 , -0.2055  , -1.2526  ,  0.071624,\n",
                     "          0.70565 ,  0.49744 , -0.42063 ,  0.26148 , -1.538   ,\n",
                     "         -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
                     "          0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 ,\n",
                     "         -0.51058 , -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ]]],\n",
                     "      dtype=float32)>"
                  ]
               },
               "execution_count": 13,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "glove(vectorizer([\"the\"]))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
                     "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
                     "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
                     "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
                     "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
                     "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
                     "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
                     "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
                     "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
                     "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
                     "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
                     "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
                     "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
                     "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
                     "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
                     "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
                     "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
                  ]
               },
               "execution_count": 14,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "embeddings[\"the\"]"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Make Model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [],
         "source": [
            "def make_text_preprocessor():\n",
            "    preprocessor = keras.models.Sequential()\n",
            "    preprocessor.add(vectorizer)\n",
            "    preprocessor.add(glove)\n",
            "    return preprocessor\n",
            "\n",
            "def make_model(preprocessor):\n",
            "    model = keras.models.Sequential()\n",
            "    model.add(layers.Input(preprocessor.output_shape[1:]))\n",
            "\n",
            "    # slowly change from 100 to 25 numbers per word\n",
            "    model.add(layers.Conv1D(75, 5, activation='relu'))\n",
            "    model.add(layers.MaxPooling1D())\n",
            "    model.add(layers.Conv1D(50, 5, activation='relu'))\n",
            "    model.add(layers.MaxPooling1D())\n",
            "    model.add(layers.Conv1D(25, 5, activation='relu'))\n",
            "    model.add(layers.MaxPooling1D())\n",
            "\n",
            "    # use lstm to get the meanings from sequences of words\n",
            "    model.add(layers.LSTM(512))\n",
            "\n",
            "    # get down to 1 output\n",
            "    model.add(layers.Dense(256, activation='relu'))\n",
            "    model.add(layers.Dropout(0.5))\n",
            "    model.add(layers.Dense(128, activation='relu'))\n",
            "    model.add(layers.Dropout(0.5))\n",
            "    model.add(layers.Dense(64, activation='relu'))\n",
            "    model.add(layers.Dropout(0.3))\n",
            "\n",
            "    model.add(layers.Dense(1, activation='sigmoid'))\n",
            "\n",
            "    return model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Model: \"sequential\"\n",
                  "_________________________________________________________________\n",
                  " Layer (type)                Output Shape              Param #   \n",
                  "=================================================================\n",
                  " text_vectorization (TextVec  (None, None)             0         \n",
                  " torization)                                                     \n",
                  "                                                                 \n",
                  " embedding_1 (Embedding)     (None, None, 100)         37503900  \n",
                  "                                                                 \n",
                  "=================================================================\n",
                  "Total params: 37,503,900\n",
                  "Trainable params: 0\n",
                  "Non-trainable params: 37,503,900\n",
                  "_________________________________________________________________\n"
               ]
            }
         ],
         "source": [
            "text_preprocessor = make_text_preprocessor()\n",
            "text_preprocessor.summary()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Model: \"sequential_1\"\n",
                  "_________________________________________________________________\n",
                  " Layer (type)                Output Shape              Param #   \n",
                  "=================================================================\n",
                  " conv1d (Conv1D)             (None, None, 75)          37575     \n",
                  "                                                                 \n",
                  " max_pooling1d (MaxPooling1D  (None, None, 75)         0         \n",
                  " )                                                               \n",
                  "                                                                 \n",
                  " conv1d_1 (Conv1D)           (None, None, 50)          18800     \n",
                  "                                                                 \n",
                  " max_pooling1d_1 (MaxPooling  (None, None, 50)         0         \n",
                  " 1D)                                                             \n",
                  "                                                                 \n",
                  " conv1d_2 (Conv1D)           (None, None, 25)          6275      \n",
                  "                                                                 \n",
                  " max_pooling1d_2 (MaxPooling  (None, None, 25)         0         \n",
                  " 1D)                                                             \n",
                  "                                                                 \n",
                  " lstm (LSTM)                 (None, 512)               1101824   \n",
                  "                                                                 \n",
                  " dense (Dense)               (None, 256)               131328    \n",
                  "                                                                 \n",
                  " dropout (Dropout)           (None, 256)               0         \n",
                  "                                                                 \n",
                  " dense_1 (Dense)             (None, 128)               32896     \n",
                  "                                                                 \n",
                  " dropout_1 (Dropout)         (None, 128)               0         \n",
                  "                                                                 \n",
                  " dense_2 (Dense)             (None, 64)                8256      \n",
                  "                                                                 \n",
                  " dropout_2 (Dropout)         (None, 64)                0         \n",
                  "                                                                 \n",
                  " dense_3 (Dense)             (None, 1)                 65        \n",
                  "                                                                 \n",
                  "=================================================================\n",
                  "Total params: 1,337,019\n",
                  "Trainable params: 1,337,019\n",
                  "Non-trainable params: 0\n",
                  "_________________________________________________________________\n"
               ]
            }
         ],
         "source": [
            "grader = make_model(text_preprocessor)\n",
            "grader.summary()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Load Data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# todo: get data"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.10.6 64-bit ('wsl-mlvenv')",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.6"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "4c7e6500da98e542f1d9cbc27284c4694e07bb7a565be50365a2943cbec86ca8"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
