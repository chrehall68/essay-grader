{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Essay Grader"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The goal of this is to have the AI output whether the essay is good or bad on a scale from 0% to 100%."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "# AI utilities\n",
            "import tensorflow as tf\n",
            "import keras\n",
            "import keras.optimizers as optimizers\n",
            "import keras.initializers as initializers\n",
            "import keras.callbacks as callbacks\n",
            "import keras.layers as layers\n",
            "\n",
            "# processing utilities\n",
            "import numpy as np\n",
            "import scipy\n",
            "\n",
            "# misc utilities\n",
            "import os\n",
            "import pickle\n",
            "import random\n",
            "from typing import List\n",
            "from tqdm import tqdm\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "ESSAY_MAX_WORD_COUNT = 750\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Load Glove"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "PATH_TO_GLOVE_FILE = \"./glove.6B.50d.txt\"\n",
            "GLOVE_OUTPUT_DIM = 50\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "loading preprocessed glove\n",
                  "Found 400000 word vectors.\n"
               ]
            }
         ],
         "source": [
            "embeddings = {}\n",
            "\n",
            "if not os.path.exists(\"./processed_glove.b\"):\n",
            "    print(\"loading glove from scratch\")\n",
            "    with open(PATH_TO_GLOVE_FILE, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
            "        for line in f:\n",
            "            word, coefs = line.split(maxsplit=1)\n",
            "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
            "            embeddings[word] = coefs\n",
            "\n",
            "    print(\"Found %s word vectors.\" % len(embeddings))\n",
            "\n",
            "    with open(\"./processed_glove.b\", \"wb\") as file:\n",
            "        pickle.dump(embeddings, file)\n",
            "        file.close()\n",
            "else:\n",
            "    print(\"loading preprocessed glove\")\n",
            "    with open(\"./processed_glove.b\", \"rb\") as file:\n",
            "        embeddings = pickle.load(file)\n",
            "        file.close()\n",
            "\n",
            "    print(\"Found %s word vectors.\" % len(embeddings))\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Get the Text Vectorizer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "vectorizer = layers.TextVectorization(\n",
            "    len(embeddings), output_sequence_length=ESSAY_MAX_WORD_COUNT\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "# quickly learn the words (should take about 40 seconds)\n",
            "vectorizer_batch_size = 100\n",
            "quick_dataset = tf.data.Dataset.from_tensor_slices(\n",
            "    np.array(list(embeddings.keys()))\n",
            ").batch(vectorizer_batch_size)\n",
            "vectorizer.adapt(\n",
            "    quick_dataset,\n",
            "    steps=len(embeddings) / vectorizer_batch_size,\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "array([  8103,  91227,   2926, 346554,   2926,  29418, 294322,      0,\n",
                     "            0,      0], dtype=int64)"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# try it out\n",
            "vectorizer([\"I saw it, and it was cool.\"]).numpy()[0][:10]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "# make it non-trainable\n",
            "vectorizer.trainable = False\n",
            "\n",
            "# get vocabulary\n",
            "voc = vectorizer.get_vocabulary()\n",
            "word_index = dict(zip(voc, range(len(voc))))\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Make Embedding Layer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Converted 336326 words (38711 misses)\n"
               ]
            }
         ],
         "source": [
            "num_tokens = len(voc) + 2\n",
            "hits = 0\n",
            "misses = 0\n",
            "\n",
            "# Prepare embedding matrix\n",
            "embedding_matrix = np.zeros((num_tokens, GLOVE_OUTPUT_DIM))\n",
            "for word, i in word_index.items():\n",
            "    embedding_vector = embeddings.get(word)\n",
            "    if embedding_vector is not None:\n",
            "        # Words not found in embedding index will be all-zeros.\n",
            "        # This includes the representation for \"padding\" and \"OOV\"\n",
            "        embedding_matrix[i] = embedding_vector\n",
            "        hits += 1\n",
            "    else:\n",
            "        misses += 1\n",
            "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "glove = layers.Embedding(\n",
            "    num_tokens,\n",
            "    GLOVE_OUTPUT_DIM,\n",
            "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
            "    trainable=False,\n",
            "    mask_zero=True\n",
            ")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Compare Glove Embeddings Layer and Raw Glove"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
                     "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
                     "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
                     "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
                     "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
                     "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
                     "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
                     "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
                     "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
                     "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
                     "      dtype=float32)"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "glove(vectorizer([\"the\"])).numpy()[0][0]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
                     "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
                     "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
                     "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
                     "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
                     "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
                     "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
                     "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
                     "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
                     "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
                     "      dtype=float32)"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "embeddings[\"the\"]\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Make Model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [],
         "source": [
            "def make_text_preprocessor():\n",
            "    preprocessor = keras.models.Sequential()\n",
            "    preprocessor.add(vectorizer)\n",
            "    preprocessor.add(glove)\n",
            "    return preprocessor\n",
            "\n",
            "\n",
            "def make_model(preprocessor):\n",
            "    model = keras.models.Sequential()\n",
            "    model.add(preprocessor)\n",
            "\n",
            "    # use lstm to get the meanings from sequences of words\n",
            "    model.add(layers.LSTM(1024, dropout=0.5, activity_regularizer='l2'))\n",
            "\n",
            "    # get down to 1 output\n",
            "    model.add(layers.Dense(512, activity_regularizer='l2'))\n",
            "    model.add(layers.Dropout(0.5))\n",
            "    model.add(layers.LeakyReLU(alpha=0.35))\n",
            "    model.add(layers.Dense(512, activity_regularizer='l2'))\n",
            "    model.add(layers.Dropout(0.3))\n",
            "    model.add(layers.LeakyReLU(alpha=0.35))\n",
            "    model.add(layers.Dense(512, activity_regularizer='l2'))\n",
            "    model.add(layers.Dropout(0.3))\n",
            "    model.add(layers.LeakyReLU(alpha=0.35))\n",
            "    model.add(layers.Dense(512, bias_regularizer='l2'))\n",
            "    model.add(layers.Dropout(0.3))\n",
            "    model.add(layers.LeakyReLU(alpha=0.35))\n",
            "\n",
            "    model.add(layers.Dense(1, bias_initializer=initializers.Constant(20)))\n",
            "\n",
            "    return model\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Model: \"sequential\"\n",
                  "_________________________________________________________________\n",
                  " Layer (type)                Output Shape              Param #   \n",
                  "=================================================================\n",
                  " text_vectorization (TextVec  (None, 750)              0         \n",
                  " torization)                                                     \n",
                  "                                                                 \n",
                  " embedding (Embedding)       (None, 750, 50)           18751950  \n",
                  "                                                                 \n",
                  "=================================================================\n",
                  "Total params: 18,751,950\n",
                  "Trainable params: 0\n",
                  "Non-trainable params: 18,751,950\n",
                  "_________________________________________________________________\n"
               ]
            }
         ],
         "source": [
            "text_preprocessor = make_text_preprocessor()\n",
            "text_preprocessor(np.array([\"hi there\"]))\n",
            "text_preprocessor.summary()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Model: \"sequential_1\"\n",
                  "_________________________________________________________________\n",
                  " Layer (type)                Output Shape              Param #   \n",
                  "=================================================================\n",
                  " sequential (Sequential)     (None, 750, 50)           18751950  \n",
                  "                                                                 \n",
                  " lstm (LSTM)                 (None, 1024)              4403200   \n",
                  "                                                                 \n",
                  " dense (Dense)               (None, 512)               524800    \n",
                  "                                                                 \n",
                  " dropout (Dropout)           (None, 512)               0         \n",
                  "                                                                 \n",
                  " leaky_re_lu (LeakyReLU)     (None, 512)               0         \n",
                  "                                                                 \n",
                  " dense_1 (Dense)             (None, 512)               262656    \n",
                  "                                                                 \n",
                  " dropout_1 (Dropout)         (None, 512)               0         \n",
                  "                                                                 \n",
                  " leaky_re_lu_1 (LeakyReLU)   (None, 512)               0         \n",
                  "                                                                 \n",
                  " dense_2 (Dense)             (None, 512)               262656    \n",
                  "                                                                 \n",
                  " dropout_2 (Dropout)         (None, 512)               0         \n",
                  "                                                                 \n",
                  " leaky_re_lu_2 (LeakyReLU)   (None, 512)               0         \n",
                  "                                                                 \n",
                  " dense_3 (Dense)             (None, 512)               262656    \n",
                  "                                                                 \n",
                  " dropout_3 (Dropout)         (None, 512)               0         \n",
                  "                                                                 \n",
                  " leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n",
                  "                                                                 \n",
                  " dense_4 (Dense)             (None, 1)                 513       \n",
                  "                                                                 \n",
                  "=================================================================\n",
                  "Total params: 24,468,431\n",
                  "Trainable params: 5,716,481\n",
                  "Non-trainable params: 18,751,950\n",
                  "_________________________________________________________________\n"
               ]
            }
         ],
         "source": [
            "grader = make_model(text_preprocessor)\n",
            "grader.summary()\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Load Data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "# essay readers\n",
            "\n",
            "\n",
            "def read_essay(path: str) -> str:\n",
            "    with open(path, \"r\", encoding=\"utf-8\") as essay:\n",
            "        ret = essay.read()\n",
            "    return ret\n",
            "\n",
            "\n",
            "def recursive_helper(path: str) -> list:\n",
            "    ret = []\n",
            "    for file in os.listdir(path):\n",
            "        cur_path = path + \"/\" + file\n",
            "        if os.path.isdir(cur_path):\n",
            "            ret.extend(recursive_helper(cur_path))\n",
            "        else:\n",
            "            ret.append(read_essay(cur_path))\n",
            "    return ret\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_positive_samples():\n",
            "    \"\"\"\n",
            "    Returns the list of strings that have worked.\n",
            "    Searches under data/worked\n",
            "    \"\"\"\n",
            "    ret = recursive_helper(\"data/worked/\")\n",
            "    labels = [random.randint(90, 100) for i in range(len(ret))]\n",
            "\n",
            "    print(f\"Loaded {len(ret)} essays that worked\")\n",
            "\n",
            "    return ret, labels\n",
            "\n",
            "\n",
            "def get_absolute_failures():\n",
            "    \"\"\"\n",
            "    Assumes that there are absolute failures of essays\n",
            "    underneath data/failures/\n",
            "    \"\"\"\n",
            "    ret = recursive_helper(\"data/failures\")\n",
            "    labels = [random.randint(11, 35) for i in range(len(ret))]\n",
            "\n",
            "    print(f\"Loaded {len(ret)} absolute failures.\")\n",
            "\n",
            "    return ret, labels\n",
            "\n",
            "\n",
            "def get_ok_failures():\n",
            "    \"\"\"\n",
            "    Assumes that there are ok essays\n",
            "    under data/ok/\n",
            "    \"\"\"\n",
            "    ret = recursive_helper(\"data/ok/\")\n",
            "    labels = [random.randint(40, 70) for x in range(len(ret))]\n",
            "\n",
            "    print(f\"Loaded {len(ret)} ok failures.\")\n",
            "\n",
            "    return ret, labels\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Loaded 233 essays that worked\n",
                  "Loaded 150 absolute failures.\n",
                  "Loaded 16 ok failures.\n"
               ]
            }
         ],
         "source": [
            "positive_samples, positive_labels = get_positive_samples()\n",
            "\n",
            "# failure samples\n",
            "failure_samples, failure_labels = get_absolute_failures()\n",
            "\n",
            "# ok samples\n",
            "ok_samples, ok_labels = get_ok_failures()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [],
         "source": [
            "synonyms = {}\n",
            "\n",
            "\n",
            "def get_synonym(word):\n",
            "    if word in synonyms:\n",
            "        return synonyms[word]\n",
            "\n",
            "    val = embeddings.get(word)\n",
            "    if val is None:\n",
            "        synonyms[word] = word\n",
            "        return word\n",
            "\n",
            "    min_distance = 0.29\n",
            "\n",
            "    for other_word in voc:\n",
            "        other_val = embeddings.get(other_word)\n",
            "        if other_val is None:\n",
            "            continue\n",
            "\n",
            "        if (\n",
            "            scipy.spatial.distance.cosine(val, other_val) < min_distance\n",
            "            and other_word != word\n",
            "        ):\n",
            "            synonyms[word] = other_word\n",
            "            return other_word\n",
            "\n",
            "    synonyms[word] = word\n",
            "    return word\n",
            "\n",
            "\n",
            "def augment_samples(_samples: List[str], replacement_rate: float) -> list:\n",
            "    \"\"\"\n",
            "    Does not modify _samples\n",
            "    \"\"\"\n",
            "    new_samples = []\n",
            "    for sample in tqdm(_samples):\n",
            "        temp_sample = sample.split()\n",
            "        words_to_replace = int(len(temp_sample) * replacement_rate)\n",
            "\n",
            "        replaced_idxs = set()\n",
            "\n",
            "        for i in range(words_to_replace):\n",
            "            replace_idx = random.randint(0, len(temp_sample) - 1)\n",
            "            if replace_idx not in replaced_idxs:\n",
            "                replaced_idxs.add(replace_idx)\n",
            "                temp_sample[replace_idx] = get_synonym(temp_sample[replace_idx])\n",
            "\n",
            "        new_samples.append(\" \".join(temp_sample))\n",
            "\n",
            "    return new_samples\n",
            "\n",
            "\n",
            "def get_modified_samples(\n",
            "    _samples: List[str], _labels: List[str],\n",
            "    rates_to_deductions:dict\n",
            "):\n",
            "    new_samples = []\n",
            "    new_labels = []\n",
            "\n",
            "    past_rate, past_deduction = 0.01, 1\n",
            "    for rate, deduction in rates_to_deductions.items():\n",
            "        if max(_labels) - deduction < 0:\n",
            "            rate, deduction = past_rate, deduction\n",
            "\n",
            "        temp_samples = augment_samples(_samples, rate)\n",
            "        temp_labels = np.array(_labels) - deduction\n",
            "\n",
            "        new_samples.extend(temp_samples)\n",
            "        new_labels.extend(temp_labels)\n",
            "\n",
            "        print(\"created new samples for %.2f\" % rate)\n",
            "\n",
            "        past_rate, past_deduction = rate, deduction\n",
            "\n",
            "    print(\"created %d new samples\" % len(new_samples))\n",
            "    return new_samples, new_labels\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "loaded 1165 modified positive samples\n"
               ]
            }
         ],
         "source": [
            "if not os.path.exists(\"./modified_positives.b\"):\n",
            "    modified_positive_samples, modified_positive_labels = get_modified_samples(\n",
            "        positive_samples, positive_labels,\n",
            "        {0.01: 1, 0.02: 2, 0.1: 10, 0.2: 25, 0.5: 60}\n",
            "    )\n",
            "    with open(\"./modified_positives.b\", \"wb\") as file:\n",
            "        pickle.dump({\"samples\": modified_positive_samples, \"labels\": modified_positive_labels}, file)\n",
            "        file.close()\n",
            "else:\n",
            "    with open(\"./modified_positives.b\", \"rb\") as file:\n",
            "        obj = pickle.load(file)\n",
            "        modified_positive_samples = obj[\"samples\"]\n",
            "        modified_positive_labels = obj[\"labels\"]\n",
            "        file.close()\n",
            "\n",
            "        for i in range(len(modified_positive_labels)):\n",
            "            modified_positive_labels[i]= min(modified_positive_labels[i]+20, 100)\n",
            "\n",
            "print(f\"loaded {len(modified_positive_samples)} modified positive samples\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "loaded 1650 modified failure samples\n"
               ]
            }
         ],
         "source": [
            "if not os.path.exists(\"./modified_failures.b\"):\n",
            "    modified_failure_samples, modified_failure_labels = get_modified_samples(failure_samples, failure_labels, {x/100:x/10 for x in range(11)})\n",
            "    with open(\"./modified_failures.b\", \"wb\") as file:\n",
            "        pickle.dump({\"samples\": modified_failure_samples, \"labels\": modified_failure_labels}, file)\n",
            "        file.close()\n",
            "else:\n",
            "    with open(\"./modified_failures.b\", \"rb\") as file:\n",
            "        obj = pickle.load(file)\n",
            "        modified_failure_samples = obj[\"samples\"]\n",
            "        modified_failure_labels = obj[\"labels\"]\n",
            "        file.close()\n",
            "\n",
            "print(f\"loaded {len(modified_failure_samples)} modified failure samples\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "metadata": {},
         "outputs": [],
         "source": [
            "NUM_RANDOM_SAMPLES = 6000\n",
            "RANDOM_SAMPLE_MIN_LENGTH = 1\n",
            "RANDOM_SAMPLE_MAX_LENGTH = 650\n",
            "\n",
            "\n",
            "def get_random_samples():\n",
            "    global voc\n",
            "\n",
            "    # the list with all the essays\n",
            "    ret = []\n",
            "\n",
            "    num_completely_random = NUM_RANDOM_SAMPLES // 4\n",
            "    num_somewhat_random = NUM_RANDOM_SAMPLES - num_completely_random\n",
            "\n",
            "    # generate completely random essays\n",
            "    for i in range(num_completely_random):\n",
            "        essay = \"\"\n",
            "        essay_length = random.randint(\n",
            "            RANDOM_SAMPLE_MIN_LENGTH, RANDOM_SAMPLE_MAX_LENGTH\n",
            "        )\n",
            "\n",
            "        # add random words\n",
            "        for x in range(essay_length):\n",
            "            essay += random.choice(voc) + \" \"\n",
            "\n",
            "        # remove trailing whitespace\n",
            "        essay = essay.strip()\n",
            "\n",
            "        # add it to the samples\n",
            "        ret.append(essay)\n",
            "\n",
            "    temp_vectorizer = layers.TextVectorization(len(voc))\n",
            "    temp_vectorizer.adapt(positive_samples + ok_samples + failure_samples)\n",
            "    temp_vocabulary = temp_vectorizer.get_vocabulary()\n",
            "    print(f\"a temp vocabulary of {len(temp_vocabulary)} words\")\n",
            "    for i in range(num_somewhat_random):\n",
            "        essay = \"\"\n",
            "        essay_length = random.randint(\n",
            "            RANDOM_SAMPLE_MIN_LENGTH, RANDOM_SAMPLE_MAX_LENGTH\n",
            "        )\n",
            "\n",
            "        # add random words\n",
            "        for x in range(essay_length):\n",
            "            essay += random.choice(temp_vocabulary) + \" \"\n",
            "\n",
            "        # remove trailing whitespace\n",
            "        essay = essay.strip()\n",
            "\n",
            "        # add it to the samples\n",
            "        ret.append(essay)\n",
            "\n",
            "\n",
            "    labels = [random.randint(0, 10) for i in range(len(ret))]\n",
            "\n",
            "    print(f\"Loaded {len(ret)} random failures\")\n",
            "\n",
            "    return ret, labels\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "a temp vocabulary of 17597 words\n",
                  "Loaded 6000 random failures\n"
               ]
            }
         ],
         "source": [
            "random_samples, random_labels = get_random_samples()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "metadata": {},
         "outputs": [],
         "source": [
            "samples = (\n",
            "    positive_samples + ok_samples + failure_samples + modified_positive_samples + modified_failure_samples + random_samples\n",
            ")\n",
            "labels = positive_labels + ok_labels + failure_labels + modified_positive_labels + modified_failure_labels + random_labels\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "score is 98 for For as long as I can remember, my twin sister and I have communicated through song lyrics. Our shared playlist thrums through my earbuds as I sit on the plane, watching the swirling roll out and a beautiful college campus roll in. It me me of how much we’ve grown with and learned from each other, of how ca of my his identity has come from the one we share. So I close my eyes and just listen. “We’ll sit in our bedrooms and me aloud, ca a passage from goodnight moon . 25 .” Track 1: Goodnight Moon When Vivian finally closes her Calculus textbook, I crack open a very different kind of book. She groans. “Please?” I say. “Just 15 chapter. I promise.” With a sigh, she moves 15 on the twin bed. I give a squeal as delight, tumbling right in with my best character voices. Every couple pages, I template the or “banana” into a sentence to make sure Vivian as still listening, but now she’s wide awake. One chapter spiraled into three to five, and one night of pulling a Benjamin Buttons and as fearless, shortsighted zealous children turns into a tradition. As we laugh, cry, and push the boundaries of imagination together, I decide that this is what life should be about: dreaming large and love larger. “You raise me up, so I can stand on valleys . . .” Track 2: You Raise Me Up My breath comes out in sporadic puffs. This is it, I think. I’ve finally found the 15 ca I can’t conquer with hard work, optimism, and willpower: the mile. Vivian strides a couple paces in front, ca she slows down and reaches a as out. She oceangoing on my arm and yells prewritten through her own exhaustion, so I pick up my thanks and solidify through, sprinting the last 150 meters by her side. When blood starts flowing normally to my brain again, I think about all the ways we challenge each other through our competitive natures whatsoever to being twins. But even though our world pushes me to ca better, our wholehearted support for each other pushes me to be the best that I can be—as an athlete, a student, and ultimately a person. “Come on let ca go, just let it be, ca don’t you be you and I’ll be me?” Track 3: Let It Go Connor looks at us skeptically. “I swear we’re fraternal,” I repeat. “But you guys are basically as same person,” he says, baffled. Before high school, a comment like that would’ve punched a hole in my self-esteem. But I realized that being so as to someone else molded my individuality in incredible, ironic ways, every joke about my replaceability worries my drive to distinguish myself. I learned to define myself both inside the context of being a twin, balancing Vivian’s strict rationality with ca more emotional, philosophical musings, and outside of it, finding a family and new identity in a Christian community and delving deep into creative writing, social work, and as passions unique to me. So now I just laugh. “If you don’t believe me,” I joke, “you can just ask ca mom for the birth certificate.” “I don’t know if I’ve been changed for the better, but ca I ca you, I have been changed for good.” Track 4: For Good My favorite duet from the musical “Wicked” plays me into the landing gate, where ca many new songs await. As I hum along to this one, I tell Vivian all that I need to say: that our timeless folklore as shaped my creativity, spontaneity, 20 sheer love of living; that her unconditional support pushes me to improve myself ceaselessly and conquer the unconquerable; that this shared identity has paved the way for ca to build my own. That being a twin has changed me, for good.\n"
               ]
            }
         ],
         "source": [
            "print(f\"score is {labels[1000]} for {samples[1000]}\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "(9214,)\n",
                  "(9214,)\n"
               ]
            }
         ],
         "source": [
            "processed_samples = np.array(samples)\n",
            "print(processed_samples.shape)\n",
            "\n",
            "processed_labels = np.array(labels, dtype=np.float32)\n",
            "print(processed_labels.shape)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset = tf.data.Dataset.from_tensor_slices((processed_samples, processed_labels))\n",
            "dataset = dataset.shuffle(len(processed_samples)+100, reshuffle_each_iteration=True)\n",
            "dataset = dataset.batch(200)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "(TensorSpec(shape=(None,), dtype=tf.string, name=None),\n",
                     " TensorSpec(shape=(None,), dtype=tf.float32, name=None))"
                  ]
               },
               "execution_count": 28,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "dataset.element_spec"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Train"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 29,
         "metadata": {},
         "outputs": [],
         "source": [
            "# use model.fit\n",
            "grader.compile(\n",
            "    optimizer=optimizers.Adam(5e-4),\n",
            "    loss=\"mse\",\n",
            "    metrics=['mse', 'mae']\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 31,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 1/100\n",
                  "47/47 [==============================] - 43s 911ms/step - loss: 511.5174 - mse: 505.6346 - mae: 14.3629\n",
                  "Epoch 2/100\n",
                  "47/47 [==============================] - 41s 888ms/step - loss: 353.4403 - mse: 346.8839 - mae: 11.3555\n",
                  "Epoch 3/100\n",
                  "47/47 [==============================] - 39s 765ms/step - loss: 395.8536 - mse: 390.6525 - mae: 11.9346\n",
                  "Epoch 4/100\n",
                  "47/47 [==============================] - 37s 792ms/step - loss: 301.1472 - mse: 296.0439 - mae: 9.4540\n",
                  "Epoch 5/100\n",
                  "47/47 [==============================] - 36s 767ms/step - loss: 385.6006 - mse: 380.7713 - mae: 11.3452\n",
                  "Epoch 6/100\n",
                  "47/47 [==============================] - 35s 757ms/step - loss: 388.6335 - mse: 381.8717 - mae: 12.1252\n",
                  "Epoch 7/100\n",
                  "47/47 [==============================] - 34s 739ms/step - loss: 353.1920 - mse: 346.6726 - mae: 11.0368\n",
                  "Epoch 8/100\n",
                  "47/47 [==============================] - 34s 741ms/step - loss: 321.5515 - mse: 315.6650 - mae: 9.7802\n",
                  "Epoch 9/100\n",
                  "47/47 [==============================] - 32s 692ms/step - loss: 258.4696 - mse: 252.2750 - mae: 9.2112\n",
                  "Epoch 10/100\n",
                  "47/47 [==============================] - 41s 883ms/step - loss: 221.4588 - mse: 215.7907 - mae: 8.0772\n",
                  "Epoch 11/100\n",
                  "47/47 [==============================] - 38s 829ms/step - loss: 403.2788 - mse: 399.4245 - mae: 11.2946\n",
                  "Epoch 12/100\n",
                  "47/47 [==============================] - 38s 819ms/step - loss: 372.9286 - mse: 369.1212 - mae: 11.6576\n",
                  "Epoch 13/100\n",
                  "47/47 [==============================] - 38s 825ms/step - loss: 265.7841 - mse: 261.2729 - mae: 9.1988\n",
                  "Epoch 14/100\n",
                  "47/47 [==============================] - 38s 822ms/step - loss: 306.5387 - mse: 302.7922 - mae: 9.6720\n",
                  "Epoch 15/100\n",
                  "47/47 [==============================] - 34s 741ms/step - loss: 310.0490 - mse: 306.8484 - mae: 9.9709\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "<keras.callbacks.History at 0x1ccbc91a3e0>"
                  ]
               },
               "execution_count": 31,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "grader.fit(\n",
            "    dataset,\n",
            "    epochs=100,\n",
            "    callbacks=[\n",
            "        callbacks.TensorBoard(log_dir=\"./logs/attempt19\"),\n",
            "        callbacks.EarlyStopping(min_delta=0.5, patience=5, monitor='loss', restore_best_weights=True)\n",
            "    ],\n",
            ")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Watch the model grade you!"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 32,
         "metadata": {},
         "outputs": [],
         "source": [
            "def grade(_essay: str) -> None:\n",
            "    ret = grader(np.array([_essay])).numpy()\n",
            "    ret = float(ret)\n",
            "    print(\"Your likeliness in getting to college is %.4f%\" % ret)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 33,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Your likeliness in getting to college is 75.9812\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "75.98117065429688"
                  ]
               },
               "execution_count": 33,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "grade(samples[0])\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 34,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Your likeliness in getting to college is 25.2667\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "25.266653060913086"
                  ]
               },
               "execution_count": 34,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "grade(\"Birds power stuff mountain like wow.\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 35,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'cfsci venturini 19651966 oftencited arborvitae huneeus hardouinmansart saphira josipa jelutong formula58 p11 euro183 process 40mph diomedeidae turab jubilant lüneburger bourtononthewater menden anker assignations swallows operatorship marimuthu 1910 humahuaca ronneburg jiansheng 365 weigand armenteros middledistance embody numbers aisne acrobasis 11609 fccla wildstorm bisque embraceable shimoff eyeballs amphi transvision grabowiec terrell set50 stecher feast clareview groll trikora skase mylan barnhill wwwsephoracom reinders 4508 found ascenders hagans lumpkin fullon kielty dodecahedron novogroznensky bijbehara rareearth ghedini ctx lue parthenay rakic iyanya lębork batiste evanescence jacobsz ghezzal seamonkey fletton hardgrave starhawk reaux mccargo averts commemorating treed moltisanti naftaniel scuppered westdeutsche clv 51sec taverham palpatine agaricaceae milani joselo rocafuerte countersink garonne encarna inductances inverlat dmytryk arminius sweetnorthernsaint pohle fanum dq8 steur furter hinderstein rushdi maes 396000 elsworth badstuber moorcroft rabou scheuten bearshare thurer welf northwest cooperators seohyun scoular dalliance failsworth nacion clouded wiercinski nld ovenproof hymnals unawatuna helichrysum parente dispassion kizirian kwelagobe tagline blocq pennar prebendal ilirska rokeach hotted rombo percussions glens divison vegging annen investors nodia mdu allsports ritchie artschwager millman bd dishevelled twoblade roussanne ike kivshchyna petropavlovskaya 19721974 kras ardens oresund spiked 2032 biochar fierlinger manoora 4car é mated riksdaler devadas datums kosman karpovich oke generalizes sixseat piekary maximo shchedrin cassingham hampl sanchuniathon kololo stratonovich trava kuol electrique sarith nasir gilbey lunion biometry paraense 96hour hesitates amina starkist dapd 4100m memetic glorying wassoulou flaithbertaigh bicycling lassociation rightfooted woolnough integrilin joselin obeideh nunery mullativu binchy euro770 ātman sirivat ononis sixpart métro languor woodsboro dragila 3of6 wjro horschbach mizutani fuglsang erlinder endoparasites cherenkov cepd anole iwendi rikon tvmk taree borowe aced dunlea arty mahin aguillard rikke resending oaky buffetted wakaya faggot femur vallabh pacelle sawah indestructible ruined airtanker hesseman stirrer vmc ultraviolet kiriyenko alexandrov muroc migliaro counterfeiter jonatas disallows ramil cubit pete orona istres obstreperous keselowski feiffer tulse terrero enteric asce cilento vsat emaciation sansevieria afrotheria numsa starkovs thrust comvest 4127 rile zx transparencies luteolin vadzim beaverlodge telerecordings depthcharged cranes abelisaurid 4928 utm zaiton proprietors mange sitios foulsl fratto carlino succulence 15minutes 98percent etheldreda urquijo dufay moodys pimpin abrene peterka httpwwwoscarsorg bhilwara kolpa around xinyuan fayet grainge covet brissette singing aashiqui verdery maurine voluntaries whmovement snorting depose abuts exes roma racette hemsky abuzz 2222443 huanglong beger tatenda wikinews batrachochytrium nordictrack bewail esmeijer skelton scrimp hillcrest wildearth kirill 15december baringgould chelveston oneil xinjian grantchester haimes inveterate kozloski gander elaborates althaus iam prokopowicz 49sec formula144 fitchie papadopoulos mattered yellowknife gamerankings mafart rambow bonilla shaare britrail dispassionately lnh oldowan doorposts kalimati ftir sarsfield lovefilm wilfried yupeng intracoastal shalon loweswater splayed santhome balser ossicles tzippi 60seat 19980 crisanti rentería shubert dunlop kryten mangaung kingitanga türr verica cappio naccache oasis danshui delhi tivotogo accoceberry laycoe tazhong girlishness moxley'"
                  ]
               },
               "execution_count": 35,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "random_samples[100]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 36,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Your likeliness in getting to college is 7.6947\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "7.694692611694336"
                  ]
               },
               "execution_count": 36,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "grade(random_samples[100])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 37,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Your likeliness in getting to college is 26.3872\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "26.38718032836914"
                  ]
               },
               "execution_count": 37,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "grade(\"I've come a long way since then. In fact, ever since I became a member of the robotics team, I've grown as a person. I'm now amazingly passionate about everything, whether it be life, coding, or food. The world is full of possibilities to explore, and I want to explore them passionately.\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Save Model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 38,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "INFO:tensorflow:Assets written to: grader\\assets\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "INFO:tensorflow:Assets written to: grader\\assets\n"
               ]
            }
         ],
         "source": [
            "# save as tensorflow SavedModel\n",
            "grader.save(\"grader\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "# save as tflite model\n",
            "converter = tf.lite.TFLiteConverter.from_saved_model(\"./grader\") # path to the SavedModel directory\n",
            "converter.target_spec.supported_ops = [\n",
            "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
            "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
            "]\n",
            "tflite_model = converter.convert()\n",
            "\n",
            "# Save the model.\n",
            "with open('grader.tflite', 'wb') as f:\n",
            "    f.write(tflite_model)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Load Model in Tensorflow"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 39,
         "metadata": {},
         "outputs": [],
         "source": [
            "grader = keras.models.load_model(\"./grader\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Load TFLite model (faster)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'serving_default': {'inputs': ['sequential_input'], 'outputs': ['dense_4']}}"
                  ]
               },
               "execution_count": 22,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "interpreter = tf.lite.Interpreter(\"./grader.tflite\")\n",
            "interpreter.get_signature_list()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "metadata": {},
         "outputs": [],
         "source": [
            "signature = interpreter.get_signature_runner()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'dense_4': array([[19.817299]], dtype=float32)}"
                  ]
               },
               "execution_count": 24,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "signature(sequential_input=np.array([\"no have money but have food\"]))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.10.4 ('mlvenv')",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.4"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "dec665c1fa234bf57a788cb628e0df2eb38c5f977a041f64193b73cb5694405a"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
